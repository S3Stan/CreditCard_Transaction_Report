{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Sumary and introduction of the study  \n",
    "\n",
    "The problem of fraudulent credit card transaction taking place is a bigger problem than most people realise it to be. The problem is not only based on the amount of money being stolen in the transactions, but more importantly the increase in the frequency or volumne of these type of transactions taking place on a daily basis. The increase in volume of these type of transactions taking place means it is very important for credit card companies to be able to identify and develop methods to decrease the number of clients that are being charged for transactions that they did not incur.\n",
    "\n",
    "_What does this data represent?_<br>\n",
    "The following data is credit card transaction data made by European credit card holders for a random day in September 2013. Out of the original 284,807 transactions that exist in teh dataset, a subset of 90,000 of these transactions is used in this study. \n",
    "\n",
    "_Purpose of analysis?_<br>\n",
    "The purpose of this study is to create models that can be used to predict wehther a transaction is fraudulent or not using the relationships that exist within the data. To achieve this we will use regression and decison tree modelling, and then artificial neural networks.\n",
    "\n",
    "_Data source:_<br>\n",
    "The dataset source can be found at: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Data study and general imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import Analysis_repo\n",
    "from Analysis_repo import sourceCode\n",
    "from Analysis_repo import notebook_import_library as importLibrary\n",
    "\n",
    "\n",
    "# Get the data form the Source code file\n",
    "try:\n",
    "    creditCard_p1_data = sourceCode.sendCreditCardData_part1() # credit card data part 1\n",
    "    #print(creditCard_p1_data.columns) # print ouut the columns\n",
    "    creditCard_p2_data = sourceCode.sendCreditCardData_part2() # credit card data part 2\n",
    "    #print(creditCard_p2_data.columns) # print out teh columns\n",
    "    creditcardData_analysis = importLibrary.pd.concat([importLibrary.pd.DataFrame(creditCard_p1_data), importLibrary.pd.DataFrame(creditCard_p2_data)]) # append the two parts of the credit card data together\n",
    "    creditcardData_analysis.reset_index(drop=True, inplace=True)\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dimension of the datset\n",
    "print(\"Rows: \", creditcardData_analysis.shape[0], \"Cols: \", creditcardData_analysis.shape[1], \"\\n\")\n",
    "\n",
    "# 2. Data types to be observed in the dataset i.e., Dataset info\n",
    "#print(\"Data type info:\",creditcardData_analysis.info())\n",
    " \n",
    "\n",
    "# 3. Checking for missing data in the dataset\n",
    "missingDataCount =  creditcardData_analysis.isna().sum()\n",
    "#print(\"Missing data per column:\", missingDataCount)\n",
    "\n",
    "\n",
    "# 4. Distribution of Class type per transaction\n",
    "class_dist = creditcardData_analysis['Class'].value_counts().head(3)\n",
    "print(\"Frequency of transaction by Class:\", class_dist, \"\\n\") \n",
    "\n",
    "# 5. Snippet of the data \n",
    "print(\"First 2 rows data snippet:\")\n",
    "creditcardData_analysis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Get transaction amount data grouped by Class\n",
    "grouped_data = creditcardData_analysis.groupby('Class')['Amount'].sum().reset_index()\n",
    "\n",
    "# 2 - Set labels\n",
    "labels = grouped_data.Class.replace(1, 'Fraud').replace(0, \"Non-Fraud\")\n",
    "sizes =  grouped_data.Amount \n",
    "\n",
    "# 3 -  Plot all the transactions data\n",
    "plt.title(\"\\n Total transactions amount by class type\")\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#66b3ff','#99ff99'])\n",
    "\n",
    "# 4 - Plot extra\n",
    "plt.legend(labels, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Plot observation_ : From this Pie Chart we can observe in terms of Euro amounts, Fraudulent transactions account for less than 0.5% of the amount in transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Get time and amount of non-fraudulent data and plot the data on a scatter plot\n",
    "Amount = creditcardData_analysis[\"Amount\"][creditcardData_analysis[\"Class\"]== 0]\n",
    "time_ = creditcardData_analysis[\"Time\"][creditcardData_analysis[\"Class\"]== 0]\n",
    "\n",
    "importLibrary.plt.scatter(time_, Amount, color='#66b3ff', label='Non-Fraudulent transaction')\n",
    "\n",
    "\n",
    "# 2 - Get time and amount of fraudulent data\n",
    "time_of_fraud = creditcardData_analysis['Time'][creditcardData_analysis[\"Class\"]== 1]\n",
    "y_additional = creditcardData_analysis['Amount'][creditcardData_analysis[\"Class\"]== 1]\n",
    "\n",
    "\n",
    "# 3 -  Plot all the transactions data\n",
    "importLibrary.plt.title('Frequency of fraudulent transaction \\n occuring in a typical day \\n')\n",
    "importLibrary.plt.scatter(time_of_fraud, y_additional, color='brown', label='Fraudulent transactions')\n",
    "importLibrary.plt.xticks(np.arange(0, max(time_) + 1, 3600), [f'{int(t/3600)}h' for t in np.arange(0, max(time_) + 1, 3600)])\n",
    "importLibrary.plt.xlabel('Time (in hours)')\n",
    "importLibrary.plt.ylabel('Transactions amount')\n",
    "\n",
    "\n",
    "# 4 - Plot extra\n",
    "importLibrary.plt.legend()\n",
    "importLibrary.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Plot observation_ : From this scatter plot the problem we are trying to address is how to accurately determine which transactions are fradulent and which are not on a consistent basis, as teh amount of fraud money is a probelm together with its volume during the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Regression Modelling\n",
    "\n",
    "In this section we will make use of a regression to predict the class type of credit card transaction. The 'Class' column in the _creditcardData-analyis_ will be used as the independent variable and all the other the columns will used as the dependent variables in out study.\n",
    "\n",
    "For this study we want to determine, hoiw effective are a are certain input as vaiable indicators in determing teh class type of the transaction. We will use to model outputs to test effectiveness of the model by focusing on its key metrics such as the $R^2$, Mean Squared Error, Mean Absolute Error to mention few.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming a linear regression model for our study\n",
    "\n",
    "#independent and dependent variables\n",
    "Variable_inputs_X = creditcardData_analysis.drop(\"Class\", axis=1) # Drop the class column\n",
    "Output_domain_Y = creditcardData_analysis['Class'] # Get the data\n",
    "\n",
    "# set seeds and generte random variables\n",
    "i = importLibrary.np.random.choice(Output_domain_Y.index, size = len(Output_domain_Y), replace= False)\n",
    "importLibrary.np.random.seed(19184914) # set seed\n",
    "\n",
    "# Get interger vlaue of 70% the training set interger set size of\n",
    "train_set_size = int(len(creditcardData_analysis) * 0.7)  \n",
    "#print(train_set_size)\n",
    "\n",
    "# train test split for the data points:\n",
    "x_train = Variable_inputs_X.loc[i][:train_set_size]\n",
    "y_train = Output_domain_Y.loc[i][:train_set_size]\n",
    "   \n",
    "x_test = Variable_inputs_X.loc[i][train_set_size:]\n",
    "y_test = Output_domain_Y.loc[i][train_set_size:]\n",
    "\n",
    "# Form a linear regression model\n",
    "full_model = importLibrary.LinearRegression().fit(x_train, y_train) #form regression model.\n",
    "full_model.coef_, full_model.intercept_\n",
    "\n",
    "# print basic statistics:\n",
    "print(\"Model Coefficients: \\n\", full_model.coef_)\n",
    "print(\"\\n Model Intercept:\", round(full_model.intercept_, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for the the training-testing data split\n",
    "# 1 - Pie chart - Comparing the weight of fraud and non fraudulent transactions \n",
    "zero_count = (creditcardData_analysis['Class'] == 0).sum()\n",
    "num_count = (creditcardData_analysis['Class'] == 1).sum()\n",
    "fraud_class_labels = ['Non-Fraudulent Transactions', 'Fraudulent transactions']\n",
    "transactions_sizes_weight = [zero_count, num_count]  # Transaction type sizes\n",
    "\n",
    "# Pie Chart creation\n",
    "importLibrary.plt.pie(transactions_sizes_weight, labels=fraud_class_labels, colors=['teal', '#66b3ff'], autopct='%importLibrary.1.1f%%', startangle=90)\n",
    "importLibrary.plt.axis('equal')\n",
    "importLibrary.plt.title('Training Split of \\n Fraudulent transactions and  \\nnon-fraudulent transactions \\n')\n",
    "\n",
    "# Show the pie chart\n",
    "importLibrary.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3.1] Testing the effectiveness of the model\n",
    "\n",
    "To test the model that has been trained, we compute statistics to help us analyze the effectiveness of the model that has been created.\n",
    "We form a regression model then trian it accoridngly. \n",
    "\n",
    "Areas of interest in our study:\n",
    "1. $R^2$ value\n",
    "2. Mean Square Error\n",
    "3. Mean Absolute Error\n",
    "4. $R^2$ range of comparison values\n",
    "5. If $R_0^2 = 0.5$ is used; Comment on the effectiveness if the model.\n",
    "6. Recommendations on the model and how can we improve the model by removing which variables?\n",
    "7. Which explanatory variable does have the minimum and maximum linear relationships with the Class variable?\n",
    "8. Improve the model by adding data to the new model. Has the new model been improved?\n",
    "9. Import the _creditCardData-prediction_ and use the model in q8 to predict the transaction Class of the new _10000_ transactions. \n",
    "    * From the prediction data how many transactions are considered Fraudulent? Compare this value with the actual fraudulent transactions in the dataset. \n",
    "10. Conclusion on findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. R^2 value\n",
    "\n",
    "# Forming OLS regression\n",
    "me_model = importLibrary.sm.OLS(y_train, x_train).fit()  \n",
    "\n",
    "# Obtain the coefficient of determination and interpet it. Evaluate the model\n",
    "y_pred = full_model.predict(x_test)\n",
    "base_model_rsquared = importLibrary.r2_score(y_test, y_pred)\n",
    "print(\"R-Squared: \" , round(base_model_rsquared, 4), \"\\n\")\n",
    "\n",
    "# Get the train and test R2 values\n",
    "r2_range = importLibrary.r2_score(y_train, me_model.predict(x_train)), importLibrary.r2_score(y_test, me_model.predict(x_test))\n",
    "r2_train_test  = r2_range\n",
    "\n",
    "# Findings:\n",
    "print(\"Meaning of R-Squared: \")\n",
    "percentage = round((base_model_rsquared * 100),2)\n",
    "print(\"Using the model,\", percentage,\"% of the variance of Class value can be explained by the explanatory variables used in the model.\")\n",
    "print(\"The training vs. testing ranges are: \",round(r2_train_test[0], 4), round(r2_train_test[1], 4), \"respectively.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Mean Square Error\n",
    "\n",
    "base_MSE_train = round(importLibrary.mean_squared_error(y_train, full_model.predict(x_train)),5)\n",
    "base_MSE_test = round(importLibrary.mean_squared_error(y_test, full_model.predict(x_test)),5)\n",
    "\n",
    "print(\"Train:\", base_MSE_train, \"Test:\", base_MSE_test)\n",
    "\n",
    "\n",
    "print(\" Mean Square Error output:\")\n",
    "print(\"Train:\", importLibrary.mean_squared_error(y_train, full_model.predict(x_train)), \"Test:\", importLibrary.mean_squared_error(y_test, full_model.predict(x_test)))\n",
    "print(\n",
    "    \" Explaination.. is .....\", '\\n',\n",
    "    \"\", '\\n'  \n",
    ")\n",
    "\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Class notes >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Mean Absolute Error\n",
    "\n",
    "base_MAE_train = round(importLibrary.mean_absolute_error(y_train, full_model.predict(x_train)),5)\n",
    "base_MAE_test = round(importLibrary.mean_absolute_error(y_test, full_model.predict(x_test)),5)\n",
    "\n",
    "print(\"Mean Absolute Error output:\")\n",
    "print(\"Train\",base_MAE_train,\"Test:\",base_MAE_test)\n",
    "print(\n",
    "    \" Explaination.. is .....\", '\\n',\n",
    "    \"\", '\\n'  \n",
    ")\n",
    "\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Class notes >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. R^2 range of comparison values\n",
    "base_R2_train = round(importLibrary.r2_score(y_train, full_model.predict(x_train)),5)\n",
    "base_R2_test = round(importLibrary.r2_score(y_test, full_model.predict(x_test)),5)\n",
    "\n",
    "print(\"R^2 value output:\")\n",
    "print(\"Train\", base_R2_train,\"Test\", base_R2_test)\n",
    "print(\n",
    "    \"The training and testing values of r2 are very close to one another.\", '\\n',\n",
    "    \"This illustrates to us that the model is either accurately working well or accurately not working well with the data.\", '\\n'  \n",
    "    \"\", '\\n'  \n",
    "    \"\", '\\n'  \n",
    ")\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Class notes >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. If R0^2 = 0.5 is used a barometer for models; Comment on the effectiveness of the model.\n",
    "r0_value = 0.5\n",
    "\n",
    "print(\n",
    "    \"Using the value of R0=\", r0_value,\"and the value of R2 =\",base_R2_test,\"as the vlaues of comparision, we can conclude that:\" '\\n',\n",
    "    \"Our model is acceptable for use since our tested R2 is greater than the R0^2 value given\", '\\n',\n",
    "    \"The model is also a good fit since the values of R2_train and R2_test are\",base_R2_train,\"and\",base_R2_train,\"respectively\", '\\n',\n",
    "    \"The narrow range between the values illustrate this to us.\", '\\n'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Recommendations on the model and how can we improve the model?\n",
    "print(\n",
    "    \"From question 5 we can deduce that the model is a good fit, however this does not mean the model cannot be improved.\", '\\n',\n",
    "    \"Possible ways of improving the model include:\", '\\n',\n",
    "    \"1. Adding more transaction data to the model.\", '\\n',\n",
    "    \"2. Removing variables that have a low linear relationship with the 'Class' variable and finding the 'right mix' of variables for our regression analysis.\", '\\n'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Which explanatory variable does have the highest and min linear relationships with the Class variable?\n",
    "\n",
    "# get the correlation series of each explanaotry variable with MedHouseVal\n",
    "explanatory_correlation_series = creditcardData_analysis.corr()['Class'].drop('Class')\n",
    "max_cor = explanatory_correlation_series.idxmax()\n",
    "min_cor = explanatory_correlation_series.idxmin()\n",
    "\n",
    "# Output\n",
    "print(\"Variables having the min and the max linear relationship with `Class` are\", min_cor, \"and\",max_cor,\"respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot of amount in inolved in fraudulent transaction per hour\n",
    "\n",
    "# 1 - Create Hour column\n",
    "hour_list = []\n",
    "    \n",
    "for time in creditcardData_analysis['Time']:\n",
    "    hour_conversion = int(time/3600) \n",
    "    hour_list.append(hour_conversion)\n",
    "\n",
    "creditcardData_analysis['Hour'] = hour_list\n",
    "\n",
    "\n",
    "# 2 - Group the data by hour\n",
    "grouped_data = creditcardData_analysis.groupby(['Hour', 'Class'])['Amount'].sum().reset_index()\n",
    "fraud_amt_hour_data = grouped_data[['Hour', 'Amount']][grouped_data[\"Class\"]== 1]\n",
    "fraud_amt_df = importLibrary.pd.DataFrame(fraud_amt_hour_data)\n",
    "\n",
    "\n",
    "# 3 - Plot data\n",
    "importLibrary.plt.title(\"Total amount involved in fraudulent transactions per hour \\n\")\n",
    "importLibrary.plt.bar(fraud_amt_df['Hour'], fraud_amt_df['Amount'], color='purple', alpha=0.65)\n",
    "importLibrary.plt.xlabel('Time (in hours)')\n",
    "importLibrary.plt.ylabel('Transactions amount (Euro)')\n",
    "\n",
    "\n",
    "# 4 - Plot extra\n",
    "importLibrary.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Improve the model by adding data to the new model. Has the new model been improved?\n",
    "\n",
    "importLibrary.np.random.seed(191849142) # set seed\n",
    "\n",
    "#independent and dependent variables\n",
    "Variable_inputs_X = creditcardData_analysis.drop([\"Class\"], axis=1)\n",
    "Output_domain_Y = creditcardData_analysis['Class']\n",
    "\n",
    "# add new data to the model; set seeds and generte random variables\n",
    "# set seeds and generte random variables\n",
    "boosted_i = importLibrary.np.random.choice(Output_domain_Y.index, size = len(Output_domain_Y), replace= False)\n",
    "\n",
    "boosted_training_set_n = int(len(creditcardData_analysis) * 0.8)\n",
    "\n",
    "boosted_x_train = Variable_inputs_X.loc[boosted_i][:boosted_training_set_n]\n",
    "boosted_y_train = Output_domain_Y.loc[boosted_i][:boosted_training_set_n]\n",
    "boosted_x_test = Variable_inputs_X.loc[boosted_i][boosted_training_set_n:]\n",
    "boosted_y_test = Output_domain_Y.loc[boosted_i][boosted_training_set_n:]\n",
    "\n",
    "# fit model with new data\n",
    "boosted_model = importLibrary.LinearRegression().fit(boosted_x_train, boosted_y_train)\n",
    "\n",
    "# Predict the R2, MAE, MSE values\n",
    "booosted_y_pred = boosted_model.predict(boosted_x_test)\n",
    "boosted_model_rsquared = round(importLibrary.r2_score(boosted_y_test, booosted_y_pred),5)\n",
    "\n",
    "boosted_MAE_train = round(importLibrary.mean_absolute_error(y_train, full_model.predict(x_train)),5)\n",
    "boosted_MAE_test = round(importLibrary.mean_absolute_error(y_test, full_model.predict(x_test)),5)\n",
    "\n",
    "boosted_MSE_train = round(importLibrary.mean_squared_error(y_train, full_model.predict(x_train)),5)\n",
    "boosted_MSE_test = round(importLibrary.mean_squared_error(y_test, full_model.predict(x_test)),5)\n",
    "\n",
    "\n",
    "# Model comparision from me_model vs boosted_model\n",
    "print(\"R-Squared comparison: \")\n",
    "print(\"The value of RSquared between models changed from \", round(base_model_rsquared,5), \"to\", boosted_model_rsquared, \"in the boosted model\")\n",
    "print(\"From the base model to the boosted model teh R2 value improved significantly from by\",round(boosted_model_rsquared - base_model_rsquared, 4) ,\"points\", \"\\n\")\n",
    "\n",
    "print(\"MSE comaprison: \")\n",
    "print(\"The test value of MSE  between models changed from\", base_MSE_test ,\"to\", boosted_MSE_test, \"in the boosted model\")\n",
    "print(\"As we can observe from the mean squared error of the model remained the same even when we added new data and boosted  the model \\n\")\n",
    "\n",
    "print(\"MAE comparison: \")\n",
    "print(\"The test value of MAE between models changed from\", base_MAE_test, \"to\", boosted_MAE_test, \"in the boosted model\")\n",
    "print(\"IN booth instances of the base and booosted model the mean absoluate error of the models remained the same\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Import the creditCardData_prediction and use the model in q8 to predict the transaction Class of the new 10000 transactions. \n",
    "\n",
    "try:\n",
    "    creditcardData_prediction_data = sourceCode.sendPredictionDataset()\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred:\", e)\n",
    "    \n",
    "mod_predict_data = creditcardData_prediction_data.drop(\"Class\", axis=1)\n",
    "mod_actual_class_data = creditcardData_prediction_data[\"Class\"]\n",
    "\n",
    "booosted_y_pred = boosted_model.predict(mod_predict_data)\n",
    "\n",
    "predict_class_list = []\n",
    "\n",
    "for value in booosted_y_pred:\n",
    "    int_value = int(value)\n",
    "    predict_class_list.append(int(value))\n",
    "    \n",
    "\n",
    "#predicted list\n",
    "#print(predict_class_list)\n",
    "\n",
    "# 9.1 - From the prediction data how many transactions are considered Fraudulent?\n",
    "pred_fraudent_transactions = len([value for value in predict_class_list if value == 1])\n",
    "print(\"The model predicts that:\", pred_fraudent_transactions, \"fraudulent transactions in the dataset\")\n",
    "\n",
    "# 9.2 -  Compare this value with the actual Fraudulent transactions in the dataset.\n",
    "actual_fraudent_transactions = len([value for value in mod_actual_class_data if value == 1])\n",
    "print(\"The actual count of class data is:\", actual_fraudent_transactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Conclusion on findings\n",
    "print(\n",
    "    \"\", '\\n',\n",
    "    \"\", '\\n',\n",
    "    \"\", '\\n',\n",
    "    \"\", '\\n'  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Decision Tree Modelling\n",
    "\n",
    "In this section I made use a decison tree model to predict if a credit card. In previous model we created a model\n",
    "using regression modelling, however as we see in our regression model conclusion the model is not accurate enough in predicting the degree of relationships that exist between the independent and dependent variables in our study.\n",
    "\n",
    "Decison tree models are more effective in uncovering relationships in linear and complex relationships that exist between the independent and dependent variables in our study.Decision tree models also handle missing data better, more versatile for classification and regression purposes.\n",
    "\n",
    "In the following cells we will implement a decision tree model and use the model to predict the degree of relationships between the independent and dependent variables in our study and test its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree modellign section imports\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decison Tree model implementation\n",
    "\n",
    "# 1- Get Class col and independent and dependent variables\n",
    "Variable_inputs_X = creditcardData_analysis.drop(\"Class\", axis=1) # Drop the class column\n",
    "Output_domain_Y = creditcardData_analysis['Class'] # Get the data\n",
    "\n",
    "\n",
    "# set seeds and generte random variables\n",
    "i = importLibrary.np.random.choice(Output_domain_Y.index, size = len(Output_domain_Y), replace= False)\n",
    "np.random.seed(42) # set seed\n",
    "\n",
    "\n",
    "# Get interger vlaue of 70% the training set interger set size of\n",
    "train_set_size = int(len(creditcardData_analysis) * 0.7)  \n",
    "#print(train_set_size)\n",
    "\n",
    "\n",
    "# train test split for the data points:\n",
    "x_train = Variable_inputs_X.loc[i][:train_set_size]\n",
    "y_train = Output_domain_Y.loc[i][:train_set_size]\n",
    "   \n",
    "x_test = Variable_inputs_X.loc[i][train_set_size:]\n",
    "y_test = Output_domain_Y.loc[i][train_set_size:]\n",
    "\n",
    "\n",
    "# create a decison tree classifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# Train the the train split\n",
    "dt_clf.fit(x_train, y_train)\n",
    "\n",
    "# 9 - Predict\n",
    "class_pred = dt_clf.predict(x_test)\n",
    "\n",
    "\n",
    "# 10 - Model accuracy\n",
    "accuracy = accuracy_score(y_test, class_pred)\n",
    "confusion = importLibrary.confusion_matrix(y_test, class_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(f'Confusion Matrix:\\n{confusion}') # explain the meaning of confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment Analysis: Write a professional analysis here >>>>>>>>>>>>>>>>>>>>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision tree model <Chnage  athe plot to a nice looking plot>\n",
    "\n",
    "\n",
    "tree_rules = export_text(dt_clf, feature_names=list(Variable_inputs_X.columns))\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment Analysis: Write a professional analysis here >>>>>>>>>>>>>>>>>>>>>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Neural networks section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<! Under construction>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Load or generate your training data\n",
    "\n",
    "\n",
    "# # get size of the datset\n",
    "# train_set_size = int(len(creditcardData_analysis) * 0.7) #make sure this value is an int for indexiing purposes\n",
    "# print(train_set_size)\n",
    "\n",
    "\n",
    "# # For this example, let's assume you have features (X_train) and labels (y_train).\n",
    "\n",
    "# # Define the neural network architecture\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(train_set_size,)),  # Define the input shape\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 units and ReLU activation\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 units and ReLU activation\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer with 1 unit (for binary classification) and sigmoid activation\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# # Evaluate the model on a test dataset\n",
    "# # X_test and y_test are your test data and labels\n",
    "# loss, accuracy = model.evaluate(x_test, y_test)\n",
    "# print(f'Test accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
